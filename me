
- I studied Informatics Engineering, and I already finished writing my thesis, but I still need to present it in front of a jury next month to obtain my degree.
The subject that I am writing about is: BI Data Modelling, applied to Procurement Processes.

Regarding work: I have been working in IT for 4 years now. I started as a Technical Consultant for an ERP provided by Oracle, were we created reports, and provided analytics of the data.

After that first job, I started working on a software development company named Globant, were I have had two main assignments.
The first one was for information security company in a Data migration project. Here we had to integrate different data sources, and used a wide range of tools from REST APIs, queues, ETLs, relational databases, different programming languages, different frameworks. All under agile methodology. The end products were some dashboards and reports, and the APIs itselves that were consumed by other clients of that company. We basically extracted and delivered whatever information needed by the business from the data.

------
Java Developer Globant Developer for Tripwire API Data Migration project. REST API developed using Groovy, Gradle and Swagger as main technologies. Satellite side products developed were: Automation Testing projects using Cucumber and Behave Driven Development frameworks, implemented in Java and Python respectively ETL project using Sprint methodology Enterprise data generator software Several Windows and Linux installers Management of data storage using several databases: MySQL, Microsoft SQL Server, Oracle DB and PostgreSQL.
------

While I was working in that project I became interested in all things data related, and I started a year long training offered by the company on Big Data.
There I learned the basic tools and technologies of the field: theory behind big data, how it works, distributed storing systems, and distributed programming, NoSQL data bases, Hadoop, MR, Yarn, Spark. During this training I had a mentor who reviewed my assignments on each of the technologies I was learning, and who I shadowed for 6 months in some big data projects.

----
Some assignments completed on this period were: Map Reduce programs setup and configuration of a cluster of nodes with various big data tools producer and consumer APIs that interacted with different data sources and queues batch and real time programming NoSQL databases management such as Hive, MongoDB, DynamoDB Pig and Sqoop jobs Worked as a Data Architect shadow for 6 months under the supervision of a mentor Worked on an internal company reseach implementing a Sentiment Analysis model to different social media accounts.
----

Since the beginning of this year I have been working as a Data Architect for an interactive education program provider called Imagine Learning, in the development of an AI chatbot. Here I entered the project at an early stage, so since joining I had to define the data needs of the project, discover the data sources, define the work needed to collect, clean and process the data into the shape and format that we needed it, and define metrics to meassure the project and present to the business stakeholders.
The first phase of the project was to train the bot using Machine Learning. Currently we are still iterating the trainings, and we are implementing a Classifier - trigger event detection (Spark).

We are using mainly Amazon technologies...



----
Data Architect Globant Data Architect for Imagine Leaning IA Chatbot. Responsible for the management of the data architecture Oversee data pipeline from end to end Implement real time and batch processing programs Define and track data related metrics Work with Amazon Web Services tools on the cloud, utilizing Kinesis Streams, Glue Crawlers, S3, Athena and Glazier Storage Make use of Quicksight reporting capabilities to obtain new insights from the data
Zeppelin
----

* Please let me know If there is anything specific you want me to talk about.


Hadoop
------
Data consolidation
Call it an "enterprise data hub" or "data lake." The idea is you have disparate data sources, and you want to perform analysis across them. This type of project consists of getting feeds from all the sources (either real time or as a batch) and shoving them into Hadoop. Sometimes this is step one to becoming a “data-driven company”; sometimes you simply want pretty reports. Data lakes usually materialize as files on HDFS and tables in Hive or Impala. There's a bold, new world where much of this shows up in HBase -- and Phoenix, in the future, because Hive is slow.

Salespeople like to say things like “schema on read,” but in truth, to be successful, you must have a good idea of what your use cases will be (that Hive schema won’t look terribly different from what you’d do in an enterprise data warehouse). The real reason for a data lake is horizontal scalability and much lower cost than Teradata or Netezza. For "analysis," many people set up Tableau and Excel on the front end. More sophisticated companies with “real data scientists” (math geeks who write bad Python) use Zeppelin or iPython notebook as a front end.


Spark - in memory analytics - streaming data
-----
Streaming analytics
Many people would call this "streaming," but streaming analytics is rather different from streaming from devices. Often, streaming analytics is a more real-time version of what an organization did in batches. Take antimoney laundering or fraud detection: Why not do that on the transaction basis and catch it as it happens rather than at the end of a cycle? The same goes for inventory management or anything else.

In some cases this is a new type of transactional system that analyzes data bit by bit as you shunt it into an analytical system in parallel. Such systems manifest themselves as Spark or Storm with HBase as the usual data store. Note that streaming analytics do not replace all forms of analytics; you’ll still want to surface historical trends or look at past data for something that you never considered.
