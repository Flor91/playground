0
00:00:00,000 --> 00:00:01,060


1
00:00:01,060 --> 00:00:03,700
In this course, we're using the Python programming interface

2
00:00:03,700 --> 00:00:05,780
to Spark called pySpark.

3
00:00:05,780 --> 00:00:08,195
pySpark provides an easy-to-use programming abstraction

4
00:00:08,195 --> 00:00:09,539
in parallel runtime.

5
00:00:09,539 --> 00:00:11,080
The way to think about it is you just

6
00:00:11,080 --> 00:00:15,040
simply say, here's an operation, run it on all of the data.

7
00:00:15,040 --> 00:00:20,420
To make all this work, RDDs are the key concept.

8
00:00:20,420 --> 00:00:24,540
A Spark program consists of two programs, a driver program

9
00:00:24,540 --> 00:00:26,210
and a workers program.

10
00:00:26,210 --> 00:00:28,960
The drivers program runs on the driver machine.

11
00:00:28,960 --> 00:00:31,350
And the worker programs run on cluster nodes

12
00:00:31,350 --> 00:00:33,220
or in local threads.

13
00:00:33,220 --> 00:00:36,120
And then RDDs are distributed across the workers.

14
00:00:36,120 --> 00:00:38,750


15
00:00:38,750 --> 00:00:42,110
The first thing a program does is to create a SparkContext

16
00:00:42,110 --> 00:00:42,990
object.

17
00:00:42,990 --> 00:00:46,260
This tells Spark how and where to access a cluster.

18
00:00:46,260 --> 00:00:48,370
pySpark shell and Databricks Cloud

19
00:00:48,370 --> 00:00:51,610
automatically create the sc variable for you.

20
00:00:51,610 --> 00:00:53,880
In iPython and other programs, you

21
00:00:53,880 --> 00:00:57,240
have to use a constructor to create a new Spark context.

22
00:00:57,240 --> 00:01:01,090
You then use the Spark context to create our RDDs.

23
00:01:01,090 --> 00:01:03,230
As a reminder, in the labs, we create

24
00:01:03,230 --> 00:01:04,800
the Spark context for you, so you

25
00:01:04,800 --> 00:01:07,860
don't have to worry about it.

26
00:01:07,860 --> 00:01:10,010
The master parameter for a Spark context

27
00:01:10,010 --> 00:01:12,580
determines which type and size of cluster to use.

28
00:01:12,580 --> 00:01:17,900
And there are two choices, local clusters and remote clusters.

29
00:01:17,900 --> 00:01:21,560
For local, you can either choose to run Spark with just one

30
00:01:21,560 --> 00:01:23,620
worker, no parallelism.

31
00:01:23,620 --> 00:01:26,190
Or you can run it with K worker threads

32
00:01:26,190 --> 00:01:27,590
where, ideally, you want to set K

33
00:01:27,590 --> 00:01:29,330
to be the same as the number of cores

34
00:01:29,330 --> 00:01:31,020
that you have in your machine.

35
00:01:31,020 --> 00:01:34,110
To work with clusters, you can either

36
00:01:34,110 --> 00:01:38,269
connect to a Spark cluster or to an Apache Mesos cluster.

37
00:01:38,269 --> 00:01:40,310
In the labs, we set the master parameter for you,

38
00:01:40,310 --> 00:01:42,833
so you don't have to worry about this parameter.

39
00:01:42,833 --> 00:01:43,333


