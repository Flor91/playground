0
00:00:00,000 --> 00:00:01,060


1
00:00:01,060 --> 00:00:04,260
pandas is an open source data analysis and modeling library

2
00:00:04,260 --> 00:00:07,160
that's an alternative to using R. pandas provides

3
00:00:07,160 --> 00:00:10,270
data frames, which are a table with named columns.

4
00:00:10,270 --> 00:00:12,570
It's the most commonly used pandas object.

5
00:00:12,570 --> 00:00:14,660
And it's represented as a Python dictionary that

6
00:00:14,660 --> 00:00:16,820
maps column names to series.

7
00:00:16,820 --> 00:00:19,460
Each pandas series object represents a column.

8
00:00:19,460 --> 00:00:21,770
And you can think of it as a one-dimensional labeled

9
00:00:21,770 --> 00:00:25,050
array that's capable of holding any data type.

10
00:00:25,050 --> 00:00:28,960
R also has a similar data frame type.

11
00:00:28,960 --> 00:00:32,360
Spark introduced DataFrames in Spark 1.3

12
00:00:32,360 --> 00:00:34,812
as an extension to RDDs.

13
00:00:34,812 --> 00:00:36,770
They're a distributed collection of data that's

14
00:00:36,770 --> 00:00:38,480
organized into named columns.

15
00:00:38,480 --> 00:00:40,910
So they're equivalent pandas in R DataFrames,

16
00:00:40,910 --> 00:00:42,210
but they're distributed.

17
00:00:42,210 --> 00:00:47,070
And the types of columns are inferred from values.

18
00:00:47,070 --> 00:00:50,140
It's very easy to convert between pandas and pySpark

19
00:00:50,140 --> 00:00:51,400
DataFrames.

20
00:00:51,400 --> 00:00:53,360
One important consideration, however,

21
00:00:53,360 --> 00:00:56,490
is that when you convert from a Spark DataFrame

22
00:00:56,490 --> 00:00:58,360
to a pandas DataFrame, you have to make sure

23
00:00:58,360 --> 00:01:00,140
that that pandas Data, Frame will

24
00:01:00,140 --> 00:01:02,250
fit in the driver's memory.

25
00:01:02,250 --> 00:01:03,760
Here's the code that you would use

26
00:01:03,760 --> 00:01:07,250
to convert the Spark DataFrame into a pandas DataFrame

27
00:01:07,250 --> 00:01:10,660
and to create a Spark DataFrame from pandas DataFrame.

28
00:01:10,660 --> 00:01:14,260
As you can see, it's very straightforward and easy to do.

29
00:01:14,260 --> 00:01:17,060
The performance difference between using DataFrames

30
00:01:17,060 --> 00:01:19,450
and RDDs is significance.

31
00:01:19,450 --> 00:01:21,390
Here is a graph that shows the performance

32
00:01:21,390 --> 00:01:23,300
of aggregating 10 million integer

33
00:01:23,300 --> 00:01:26,550
pairs in seconds on the x-axis.

34
00:01:26,550 --> 00:01:30,220
And it shows the performance in blue,

35
00:01:30,220 --> 00:01:34,410
RDDs using Python and Scala bindings and, in green,

36
00:01:34,410 --> 00:01:38,060
performance using Python and Scala bindings with DataFrames.

37
00:01:38,060 --> 00:01:40,710
And you can see that performance with DataFrames

38
00:01:40,710 --> 00:01:43,470
is almost five times better than the performance

39
00:01:43,470 --> 00:01:46,320
using RDDs for pySpark.

40
00:01:46,320 --> 00:01:48,720
So when you have a choice between using DataFrames

41
00:01:48,720 --> 00:01:51,950
and using RDDs for large data, you're better off

42
00:01:51,950 --> 00:01:55,510
using DataFrames in most cases.

